{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d2dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "import pandas\n",
    "import tensorflow_hub as hub\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827a1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence encoder\n",
    "sentence_encoder = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
    "embed = hub.load(sentence_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42985181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load('kmeans_shakespeare.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42299b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: We know what we are, but know not what we may be | predicted topic: 4\n"
     ]
    }
   ],
   "source": [
    "# Example of how to format data for embedding and prediction\n",
    "\n",
    "sample_text_string = \"We know what we are, but know not what we may be\"\n",
    "sample_text_dataframe = pandas.DataFrame({sample_text_string})\n",
    "# Run sample text through embeddinng model\n",
    "embedding_sample = embed(sample_text_dataframe[0]).numpy().tolist() \n",
    "# Predict cluster that the text belongs to\n",
    "predicted_topic = model.predict(embedding_sample)[0]\n",
    "\n",
    "print('input text: {} | predicted topic: {}'.format(sample_text_string, predicted_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526b5167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: Give every man thy ear, but few thy voice | predicted topic: 3\n"
     ]
    }
   ],
   "source": [
    "# Another example to compare model output for a different input string\n",
    "\n",
    "sample_text_string_2 = \"Give every man thy ear, but few thy voice\"\n",
    "sample_text_dataframe_2 = pandas.DataFrame({sample_text_string_2})\n",
    "# Run sample text through embeddinng model\n",
    "embedding_sample_2 = embed(sample_text_dataframe_2[0]).numpy().tolist() \n",
    "# Predict cluster that the text belongs to\n",
    "predicted_topic_2 = model.predict(embedding_sample_2)[0]\n",
    "\n",
    "print('input text: {} | predicted topic: {}'.format(sample_text_string_2, predicted_topic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7313bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanTextDoFn(beam.DoFn):\n",
    "  \"\"\"Replace tabs and remove empty lines from each input line.\"\"\"\n",
    "  def process(self, element):\n",
    "    \"\"\" Iterate over each input and apply embedding\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    cleaned_element = element.replace('\\t', ' ')\n",
    "    \n",
    "    if len(cleaned_element) > 1:\n",
    "        yield cleaned_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cffd2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDoFn(beam.DoFn):\n",
    "  \"\"\"Generate embedding from each input line.\"\"\"\n",
    "  def process(self, element):\n",
    "    \"\"\" Iterate over each input and apply embedding\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    input_dataframe = pandas.DataFrame({element})\n",
    "    input_embedding = embed(input_dataframe[0]).numpy().tolist() \n",
    "    \n",
    "    embedding = {\n",
    "        'input_text': element,\n",
    "        'input_embedding': input_embedding\n",
    "    }\n",
    "    \n",
    "    if len(element) > 0:\n",
    "        yield embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4acad6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictTopicDoFn(beam.DoFn):\n",
    "  \"\"\"Predict Topic from each embedding.\"\"\"\n",
    "  def process(self, element):\n",
    "    \"\"\" Iterate over each input and apply embedding\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    input_text = element['input_text']\n",
    "    input_embedding = element['input_embedding']\n",
    "    \n",
    "    predicted_topic = model.predict(element['input_embedding'])\n",
    "    \n",
    "    predicted_topic = {\n",
    "        'text': input_text,\n",
    "        'predicted_topic': predicted_topic[0]\n",
    "    }\n",
    "    \n",
    "    yield predicted_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a84928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(argv=None):\n",
    "  \"\"\"Main entry point; defines and runs the prediction pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='./data/kinglear.txt',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      default='output.txt',\n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "\n",
    "    predictions = (\n",
    "        p \n",
    "        | 'Read' >> ReadFromText(known_args.input)\n",
    "        | 'Clean' >> (beam.ParDo(CleanTextDoFn()))\n",
    "        | 'Embed' >> (beam.ParDo(EmbeddingDoFn()))\n",
    "        | 'Predict Topic' >> (beam.ParDo(PredictTopicDoFn()))\n",
    "    )\n",
    "\n",
    "    predictions | 'Write' >> WriteToText(known_args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3de4ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/steve/Library/Jupyter/runtime/kernel-50d58087-5c9b-414a-9091-d15be6fcfc2f.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
